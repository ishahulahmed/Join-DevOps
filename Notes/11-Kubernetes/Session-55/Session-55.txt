Docker High Level Design
-------------------------
1. build the image
2. run the image

problems in Docker
---------------------
1 pg --> 10 rooms

power cut, water cut

10 pg --> 1 pg water cut

he will shift persons to other pg temporary.

1 host --> 3 containers if host crash
no LB

scalbility
-------------
noraml servers with R53
traffic increases, multiple servers and keep LB and update R53 as LB URL..
if container crash, docker is not able to replace automatically --> not reliable --> self healing
no proper volume management (if host crash storage will also be lost i.e volumes created in docker will also be lost)
no strong network connectivity (3 hosts --> 1 conatiner in 1 host cannot connect with another container in another host)
no auto scaling
no secrets and config management

3 hosts --> 1 pod is host-1 another pod is in host-2 (here container in terms of docker but he used pod mistakenly)

docker = suit = image build + image pull through its hub + running the images as containers (docker is like a suite)
docker is providing runtime environment to run docker images...


client = docker command

docker daemon = docker engine

daemon/server responds to client

docker run nginx  --> daemon checks image available in local or not, if not available pull it and keep it in local
run a continer and send output to client

architeture is nothing but how your system is getting request and how it is processing it inside and how your sending your response back to the client.
(netflix architecture for example)

https://docs.docker.com/guides/docker-overview/

What is Docker Architectures?
-------------------------------
Docker architecture consists of docker client. docker client which is a docker command line tool and docker engine that is nothing but docker host and the local registry and the central registry. whenever, you run a command in docker host it will connect to the daemon it will check for the image in local if it is not available it will pull from the cetral repository and keep it in local create a container out of it and send a response to the client. which is nothing but command line. Apart from this we have docker objects like docker images, containers, networking and volumes.

kubernetes was using docker internally to run containers but since it was not following some principles it stopped using docker for running container. But, docker has rich interface to build images. so, we are using docker to build images. For, running containers we are using kuberenetes now.

kubernetes is a container orchestrator... (in music one person giving instructions to every musician)

a) In General Use: More broadly, an orchestrator can be anyone who organizes and coordinates various elements to achieve a specific goal. This could apply  
                   to event planning, project management, or any situation where multiple components need to be synchronized. if you want to run 100 pg, we 
                   will take help of manager..he needs some software

b) In Technology and Computing: An orchestrator refers to a tool or system that manages and coordinates the various components of a complex system. For 
                                instance, in cloud computing or container management (like Kubernetes), an orchestrator helps manage tasks like deployment,  
                                scaling, and networking of applications.

if you want to run 100pg, we take help of manager..he needs some software (Id for every PG, Room No for every pg for monitoring and control with software)
101-PG1

can 100 IIT grade developers work without manager?
(like an orchestrator in music he cannot play musical instruments but he will give instructions and guide them and connect everyone)

image buiding is not kubernetes responsibilty but container orchestration is only the resposibility of kubernetes. so, from now we build images through docker and run containers through kubernetes. from now on we don't need docker run command.

In Master Node desing master will get the request. master will decide where it should send the request.

Master/Control-Plane --> Amazon EKS 
EKS Nodes --> EC2 Instances (Hosts) with Containers

Manager will bring work ultimately who will work Team Members.
Master will decide who and how they will run. ultimately work is done by nodes. i.e nodes are running the containers.
Nodes should run containers. container run time environment will be present in node.
in order to run containers it should have container run time environment. EKS Nodes will automatically come with container run time environment.

kubectl --> A command line to connect to Kuberenetes

image, ports, versions, volumes, name, env variables (main things we will give in running docker image in docker run command)

git is a server we are communicating to git through git command line
kubernetes is a server kubectl is command line to connect to it.

we can create cluster manually in EKS. But we will use terraform DevOps Engineer. But terraform little bit complex. if we concentrate on terraform now then overall intention of learning kubernetes will be gone. ulimately in our course how to create EKS Cluster and how to upgrade it is also there using blue green deployment zero downtime. But, our intention is learning kubernetes first. EKS Team has provided eksctl (eks control) command line through which we can easily create a cluster.
eksctl --> The official CLI for Amazon EKS.
https://eksctl.io/

ChatGPT -- Cluster
-------------------
In AWS EKS (Elastic Kubernetes Service), a "cluster" refers to a set of Kubernetes resources that are managed together. Here's a breakdown of what a cluster entails in this context:

a) Control Plane: This is the managed part of the cluster that AWS takes care of. It includes components like the Kubernetes API server, scheduler, and 
                  controller manager. The control plane manages the state of the cluster and orchestrates the various operations.

b) Worker Nodes: These are the EC2 instances (or AWS Fargate, if you're using it) where your containerized applications actually run. The worker nodes are 
                 registered with the control plane and run Kubernetes components like the kubelet and container runtime.

c) Node Groups: AWS EKS uses node groups to manage and scale the worker nodes. You can define different node groups for different types of workloads, with 
                varying instance types and scaling policies.

d) Networking: The cluster uses a Virtual Private Cloud (VPC) for networking. It has its own network configurations, including subnets, security groups, and 
               network policies, to control how resources within the cluster communicate with each other and with external services.

e) Kubernetes Resources: Within the cluster, you define and manage Kubernetes resources such as Pods, Services, Deployments, and ConfigMaps. These resources 
                         represent your applicationâ€™s components and their configurations.

f) Cluster Configuration: AWS EKS allows you to configure your cluster using Kubernetes tools and APIs, and you can manage it using the AWS Management 
                          Console, AWS CLI, or Kubernetes command-line tools like kubectl.

By handling the control plane management and integration with AWS services, AWS EKS simplifies running Kubernetes at scale while letting you focus on deploying and managing your applications.


if you are using eksctl you need to cofigure it. (i.e only through your credentials it can built cluster on EKS Just like terraform need credentials)
in projects they will create some workstations and jumphosts from there you will have access to eksctl and kubectl not from laptops (off course from laptops you can have but in projects they will not give access through laptops)

Docker is not a command that is a complete suit. that's we are not installing docker in our laptops

First we need to create a workstation everything is going to be here. (nothing but ec2 instance) 

eksctl --> create and manage EKS cluster
kubectl --> to manage containers in kubernetes cluster

eksctl install
https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-eksctl.html
(just google eksctl install and go to aws documentation link in that use linux)

kubectl install
https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html
(just google kubectl install and go to aws documentation link in that use linux)

move kubectl to /usr/local/bin/ so that any user can use it
sudo mv kubectl /usr/local/bin/
kubectl version --client

after, installing those type aws configure it will ask access key and secret key and region (us-east default nothing).
or else, simply select the instance --> Actions --> Security --> IAM role --> give any role you have created with admin access.

use eksctl documentation to write eks.yaml file

managedNodeGroups:
 - name: spot
   instanceType: m5.large
   desiredCapacity: 3
   spot: true

Usually AWS will manage Control Plane, But we are asking here to manage NodeGroups also.

100 CPU, 2TB RAM

on demand --> t3.micro -- 0.02$ USD

50 CPU, 1TB RAM --> 50 CPU 1TB RAM (Spot) --> 80% discount --> AWS will take over our instances with 2min notice.

ChatGPT
-------
Here's a concise comparison of AWS On-Demand, Spot, and Reserved Instances:

1) On-Demand Instances:
   a) Pricing: Pay-as-you-go model; no long-term commitment, costs are billed per hour or second.
   b) Availability: Always available and can be launched or terminated at any time with guaranteed uptime.
   c) Use Case: Best for applications with unpredictable workloads or for short-term, urgent needs where you need immediate access without commitment.
2) Spot Instances:
   a) Pricing: Much cheaper than On-Demand (up to 90% discount), but pricing varies and can change based on supply and demand.
   b) Availability: AWS can terminate Spot Instances with short notice (usually two minutes) if capacity is needed elsewhere or if the Spot price exceeds  
                    your bid.
   c) Use Case: Ideal for flexible and fault-tolerant workloads that can tolerate interruptions, such as batch processing or large-scale data analysis.
3) Reserved Instances:
   a) Pricing: Significant discounts (up to 75%) compared to On-Demand pricing, with options for 1-year or 3-year terms; requires upfront or partial upfront  
            payment.
   b) Availability: Reserved capacity is guaranteed for the term of the reservation, providing consistent performance and availability.
   c) Use Case: Best for predictable, steady-state workloads where you can commit to a longer-term use, like running a web server or database.

spot instances are not for production they are for non production environmnet UAT and Development and Practise Purposes.
we are using spot becuase m5.large means huge bill

kubectl get nodes --> display nodes in k8 cluster


K8 Resources
-----------------
Namespace --> isolated project where you can create resources related to your project

kind:
apiVersion:
metadata:

spec:

kubectl apply -f <file-name.yaml>
kubectl delete -f <file-name.yaml>

pod is smallest deployable unit in kubernetes.

pod vs container
-------------------
a pod contains multiple containers. containers inside pod share same n/w and storage.

docker run nginx


docker suit = build + image maintanance + docker run time(containerd)